{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 1 : Imports\n",
    "\n",
    "#Import de PyTorch pour manipuler les tenseurs et les modèles\n",
    "import torch\n",
    "\n",
    "#Import de modèles CNN pré-entraînés comme ResNet, Inception, etc.\n",
    "import torchvision.models as models\n",
    "\n",
    "#Import de fonctions de transformation d'image (resize, normalisation, etc.)\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#PIL (Python Imaging Library) pour ouvrir les fichiers image\n",
    "from PIL import Image\n",
    "\n",
    "#Matplotlib pour afficher les images dans le notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import nécessaire pour manipuler les chemins de fichiers\n",
    "from pathlib import Path \n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b93488d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition des dossiers d'entrée et de sortie pour les images et les features extraits\n",
    "\n",
    "#Dossier contenant les images brutes (Flickr8k dataset)\n",
    "image_folder = Path(\"../data/raw/Flicker8k_Dataset\")\n",
    "\n",
    "#Dossier où seront enregistrées les features globales extraites par InceptionV3\n",
    "global_out = Path(\"../data/processed/features_resnet_global\")\n",
    "\n",
    "#Dossier où seront enregistrées les features spatiales (avant pooling) extraites par Inception V3\n",
    "spatial_out = Path(\"../data/processed/features_resnet_spatial\")\n",
    "\n",
    "#Création des dossiers de sortie s'ils n'existent pas déjà\n",
    "global_out.mkdir(parents=True, exist_ok=True)\n",
    "spatial_out.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "debb5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choix du device : GPU (cuda) si disponible, sinon CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "002c38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charger InceptionV3 complet pré-entraîné (avec aux_logits=True requis pour ce modèle)\n",
    "full_inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT, aux_logits=True)\n",
    "\n",
    "#Mettre sur device et mode évaluation\n",
    "full_inception = full_inception.to(device).eval()\n",
    "\n",
    "#Créer une version tronquée du modèle pour extraire les features spatiales\n",
    "#Ici on prend tous les modules jusqu'à (et incluant) 'Mixed_7c' qui correspond à l'index 15 dans children()\n",
    "#Note : 'full_inception.children()' renvoie un itérateur sur les modules du modèle dans l'ordre\n",
    "inception_spatial = torch.nn.Sequential(*list(full_inception.children())[:15])\n",
    "inception_spatial = inception_spatial.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a41ad537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations adaptées à InceptionV3\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((299, 299)),       \n",
    "    #Redimensionne l’image à 299x299 pixels.\n",
    "    #InceptionV3 a été entraîné avec cette taille comme entrée (au lieu de 224x224 pour ResNet).\n",
    "    #Si tu mets une taille différente, le modèle ne fonctionnera pas correctement.\n",
    "\n",
    "    transforms.ToTensor(),               \n",
    "    #Convertit l’image PIL (format image classique) en tenseur PyTorch.\n",
    "    #Résultat : image sous forme de tenseur [C, H, W] avec valeurs entre 0 et 1 (float32).\n",
    "    #PyTorch a besoin de cette structure pour les CNN.\n",
    "\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    #Normalise chaque canal (R, G, B) avec la moyenne et l’écart-type utilisés pour ImageNet.\n",
    "    #Pourquoi ? Le modèle a été entraîné avec des images normalisées → il s’attend à des entrées avec la même \"distribution\".\n",
    "    #Ça stabilise et accélère l’apprentissage et l’inférence.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "73fbe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#InceptionV3 a les sous-modules listés dans `full_inception.children()`\n",
    "#Mais ici on veut récupérer la sortie de Mixed_7c, on peut créer un forward hook dessus\n",
    "\n",
    "extracted_spatial_features = None\n",
    "\n",
    "def spatial_hook(module, input, output):\n",
    "    global extracted_spatial_features\n",
    "    extracted_spatial_features = output  #shape attendue (1, 2048, 8, 8)\n",
    "\n",
    "#Attacher hook à la couche Mixed_7c\n",
    "hook_handle = full_inception.Mixed_7c.register_forward_hook(spatial_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fa9bacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sélectionner un sous-échantillon des images (ici les 10 premières .jpg)\n",
    "image_list = list(image_folder.glob(\"*.jpg\"))[:10]\n",
    "\n",
    "#Pourquoi ? → pour tester ton pipeline sans traiter tout le dataset\n",
    "#Utilise une limite faible au début pour gagner du temps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bd525fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prendre un sous-échantillon (par exemple 10 images)\n",
    "image_list = list(image_folder.glob(\"*.jpg\"))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b9d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction features InceptionV3:  40%|████      | 4/10 [00:00<00:00, 17.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 2387197355_237f6f41ee\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.55176234 0.45867205 0.33790833 0.3100482  0.03113861]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.         0.         0.         0.14213018 0.        ]\n",
      "Image: 2609847254_0ec40c1cce\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.6243857  0.81712115 0.6208783  0.20508268 0.3104276 ]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.01160201 0.         0.3181781  0.         0.        ]\n",
      "Image: 2046222127_a6f300e202\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.10810361 0.24628781 0.37049508 0.36082956 0.05471717]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.         0.         0.         0.23917572 0.10675706]\n",
      "Image: 2853743795_e90ebc669d\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.13874139 0.54201263 0.1427205  0.3965921  0.14702237]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0. 0. 0. 0. 0.]\n",
      "Image: 2696951725_e0ae54f6da\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.5147253  0.31265798 0.6743825  0.5332631  0.97406775]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.14673546 0.         1.2130646  1.3432384  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction features InceptionV3: 100%|██████████| 10/10 [00:00<00:00, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 3421131122_2e4bde661e\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.13459723 0.3667224  0.28416947 0.1908385  0.06326994]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.         0.06548527 0.6173545  0.08319648 0.        ]\n",
      "Image: 3229730008_63f8ca2de2\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.30081764 0.40682164 0.12245138 0.5416552  0.6263184 ]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.939492 0.       0.       0.       0.      ]\n",
      "Image: 3220009216_10f088185e\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.44251522 0.43607864 0.10273524 0.40928122 0.97594726]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.12612928 0.         0.         0.         0.16534363]\n",
      "Image: 3415578043_03d33e6efd\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.3312826  0.09013668 0.54967326 0.13446754 0.5135267 ]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0.         0.         0.         0.         0.65142465]\n",
      "Image: 3437273677_47d4462974\n",
      " - Global feature shape: torch.Size([2048])\n",
      " - Spatial feature shape: torch.Size([64, 2048])\n",
      " - Exemple valeurs globales (5 premières): [0.27077812 1.1288655  0.5502459  0.09550028 0.8137071 ]\n",
      " - Exemple valeurs spatiales (patch 0, 5 premières dim): [0. 0. 0. 0. 0.]\n",
      "Extraction terminée, features globales et spatiales sauvegardées !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for img_path in tqdm(image_list, desc=\"Extraction features InceptionV3\"):\n",
    "\n",
    "        #Charger image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        #Reset la variable spatiale\n",
    "        extracted_spatial_features = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = full_inception(tensor)  #output logits (1000,), pas utilisé\n",
    "\n",
    "            #Extracted_spatial_features doit être rempli par hook sur Mixed_7c\n",
    "            spatial_feat = extracted_spatial_features.squeeze(0)  # (2048, 8, 8)\n",
    "            spatial_feat = spatial_feat.permute(1, 2, 0).reshape(-1, 2048)  # (64, 2048)\n",
    "\n",
    "            #Feature globale par avgpool sur la feature map spatiale\n",
    "            global_feat = torch.nn.functional.adaptive_avg_pool2d(extracted_spatial_features, (1, 1)).squeeze()  # (2048,)\n",
    "\n",
    "        #Convertir en numpy\n",
    "        global_feat_np = global_feat.cpu().numpy()\n",
    "        spatial_feat_np = spatial_feat.cpu().numpy()\n",
    "\n",
    "        #Affichage des tailles\n",
    "        print(f\"Image: {img_path.stem}\")\n",
    "        print(f\" - Global feature shape: {global_feat.shape}\")         #(2048,)\n",
    "        print(f\" - Spatial feature shape: {spatial_feat.shape}\")       #(64, 2048)\n",
    "        print(f\" - Exemple valeurs globales (5 premières): {global_feat_np[:5]}\")\n",
    "        print(f\" - Exemple valeurs spatiales (patch 0, 5 premières dim): {spatial_feat_np[0, :5]}\")\n",
    "\n",
    "        #Sauvegarder\n",
    "        image_id = img_path.stem\n",
    "        np.save(global_out / f\"{image_id}.npy\", global_feat_np)\n",
    "        np.save(spatial_out / f\"{image_id}.npy\", spatial_feat_np)\n",
    "\n",
    "finally:\n",
    "    hook_handle.remove()\n",
    "\n",
    "print(\"Extraction terminée, features globales et spatiales sauvegardées !\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
