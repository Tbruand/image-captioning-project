{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d031965",
   "metadata": {},
   "source": [
    "# 🧠 Plan du notebook - `05_training.ipynb`\n",
    "\n",
    "| Étape | Objectif                                                                                     |\n",
    "| ----- | -------------------------------------------------------------------------------------------- |\n",
    "| 1     | ⚙️ Imports des librairies, configuration du device, définition des hyperparamètres           |\n",
    "| 2     | 📁 Chargement des `features .pt` et des captions nettoyées (`cleaned_captions_dict.pkl`)     |\n",
    "| 3     | 🧠 Chargement du tokenizer (`tokenizer.pkl`) + affichage vocab                               |\n",
    "| 4     | 🔀 Split des IDs en `train` / `val` / `test` (80/10/10)                                      |\n",
    "| 5     | 🔢 Création des dictionnaires `train_captions_dict`, etc.                                    |\n",
    "| 6     | 🧱 Instanciation des `ImageCaptionDataset` pour chaque split                                 |\n",
    "| 7     | 📦 Création des `DataLoader` pour train / val / test                                         |\n",
    "| 8     | 👁️ Visualisation d’un batch (features, caption tokenisée et décodée)                        |\n",
    "| 9     | 🧠 Définition du `DecoderWithAttention` (embedding + LSTM + attention additive)              |\n",
    "| 10    | 📉 Définition de la loss (`CrossEntropy` avec padding) et de l’optimizer (`Adam`, scheduler) |\n",
    "| 11    | 🔁 Boucle d'entraînement avec évaluation sur validation à chaque epoch                       |\n",
    "| 12    | 🧪 Calcul des métriques BLEU-3 et ROUGE-L après chaque epoch                                 |\n",
    "| 13    | 📈 Visualisation des courbes (loss, BLEU, LR)                                                |\n",
    "| 14    | 💾 Sauvegarde du meilleur modèle (`state_dict`) + export des métriques                       |\n",
    "| 15    | 🎯 Évaluation finale sur le test set + génération de légendes + BLEU/ROUGE                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c372e",
   "metadata": {},
   "source": [
    "## | Étape 1 | 📁 Chargement des features .pt et des captions alignées |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bc4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Classe Tokenizer (doit être définie avant pickle.load)\n",
    "class Tokenizer:\n",
    "    def __init__(self, word2idx):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.start_token = \"<start>\"\n",
    "        self.end_token = \"<end>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "\n",
    "        self.pad_token_id = self.word2idx[self.pad_token]\n",
    "        self.start_token_id = self.word2idx[self.start_token]\n",
    "        self.end_token_id = self.word2idx[self.end_token]\n",
    "        self.unk_token_id = self.word2idx[self.unk_token]\n",
    "\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def encode(self, caption, add_special_tokens=True):\n",
    "        tokens = caption.strip().split()\n",
    "        token_ids = [self.word2idx.get(token, self.unk_token_id) for token in tokens]\n",
    "\n",
    "        if add_special_tokens:\n",
    "            return [self.start_token_id] + token_ids + [self.end_token_id]\n",
    "        else:\n",
    "            return token_ids\n",
    "\n",
    "    def decode(self, token_ids, remove_special_tokens=True):\n",
    "        words = [self.idx2word.get(idx, self.unk_token) for idx in token_ids]\n",
    "        if remove_special_tokens:\n",
    "            words = [w for w in words if w not in [self.pad_token, self.start_token, self.end_token]]\n",
    "        return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ef93ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Vocab size : 1204\n",
      "✅ Captions alignées pour 8091 images\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# 📁 Dossiers\n",
    "features_dir = Path(\"../data/processed/features_resnet_global\")\n",
    "captions_file = \"../data/raw/Flickr8k_text/Flickr8k.token.txt\"\n",
    "tokenizer_path = \"../data/vocab/tokenizer.pkl\"\n",
    "\n",
    "# 🧠 Chargement du tokenizer\n",
    "with open(tokenizer_path, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# 🔢 Taille du vocabulaire\n",
    "print(\"🧠 Vocab size :\", tokenizer.vocab_size)\n",
    "\n",
    "# 📖 Chargement des captions alignées (output de 04_)\n",
    "captions_dict_path = Path(\"../data/processed/aligned_captions.json\")\n",
    "\n",
    "with open(captions_dict_path, \"r\") as f:\n",
    "    aligned_captions = json.load(f)\n",
    "\n",
    "print(f\"✅ Captions alignées pour {len(aligned_captions)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c15f0",
   "metadata": {},
   "source": [
    "## Étape 2 – 🔄 Construction de la liste enrichie (feature_path, caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc592913",
   "metadata": {},
   "source": [
    "On vas créer la liste complète des couples `(image_id_augmenté, caption)` à partir du dictionnaire `aligned_captions.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28daa22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Captions associées à 1000268201_693b08cb0e :\n",
      " ➤ A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      " ➤ A girl going into a wooden building .\n",
      " ➤ A little girl climbing into a wooden playhouse .\n",
      " ➤ A little girl climbing the stairs to her playhouse .\n",
      " ➤ A little girl in a pink dress going into a wooden cabin .\n",
      "\n",
      "✅ Nombre total de paires (features, captions) : 161,820\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# 📖 Chargement des captions alignées (sortie du 04_)\n",
    "captions_dict_path = Path(\"../data/processed/aligned_captions.json\")\n",
    "\n",
    "with open(captions_dict_path, \"r\") as f:\n",
    "    captions_dict = json.load(f)\n",
    "\n",
    "# ✅ Vérif d’un ID d’image + 5 captions associées\n",
    "example_id = next(iter(captions_dict))\n",
    "print(f\"📝 Captions associées à {example_id} :\")\n",
    "for cap in captions_dict[example_id]:\n",
    "    print(\" ➤\", cap)\n",
    "\n",
    "# 🔄 Génération des ID augmentés\n",
    "augmentations = [\"\", \"_aug0\", \"_aug1\", \"_aug2\"]\n",
    "full_pairs = []\n",
    "\n",
    "for image_id, captions in captions_dict.items():\n",
    "    for suffix in augmentations:\n",
    "        full_id = image_id + suffix\n",
    "        for caption in captions:\n",
    "            full_pairs.append((full_id, caption))\n",
    "\n",
    "print(f\"\\n✅ Nombre total de paires (features, captions) : {len(full_pairs):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e9bbe",
   "metadata": {},
   "source": [
    "## Étape 3 – 🧱 Définition de la classe `ImageCaptionDataset`\n",
    "\n",
    "Cette classe hérite de `torch.utils.data.Dataset` et va :\n",
    "\n",
    "- charger les fichiers `.pt` (features),\n",
    "\n",
    "- encoder les captions avec le `Tokenizer`,\n",
    "\n",
    "- appliquer une troncature `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5901e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(r\"[^a-zA-Z0-9'\\s]\", \"\", caption)  # garde lettres, chiffres, apostrophes, espaces\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption)\n",
    "    return caption.strip()\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, pairs, features_dir, tokenizer, max_length=37):\n",
    "        self.pairs = pairs\n",
    "        self.features_dir = Path(features_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, caption = self.pairs[idx]\n",
    "\n",
    "        # 📦 Chargement des features\n",
    "        feature_path = self.features_dir / f\"{image_id}.pt\"\n",
    "        features = torch.load(feature_path)\n",
    "\n",
    "        # 🔡 Encodage de la légende\n",
    "        caption = clean_caption(caption) \n",
    "        encoded = self.tokenizer.encode(caption)\n",
    "        encoded = encoded[:self.max_length]  # 🔪 Troncature\n",
    "        encoded_tensor = torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "        return features, encoded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc730f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Longueur du dataset : 161820\n",
      "📐 Feature vector shape : torch.Size([2048])\n",
      "🧾 Caption encodée : [1, 4, 43, 5, 4, 91, 171, 8, 120, 54, 4, 397, 13, 394, 5, 29, 3, 694, 2]\n",
      "🧾 Caption décodée : a child in a pink dress is climbing up a set of stairs in an <unk> way\n"
     ]
    }
   ],
   "source": [
    "# 🧪 Test rapide\n",
    "dataset = ImageCaptionDataset(pairs=full_pairs,\n",
    "                              features_dir=\"../data/processed/features_resnet_global\",\n",
    "                              tokenizer=tokenizer)\n",
    "\n",
    "print(\"📦 Longueur du dataset :\", len(dataset))\n",
    "\n",
    "features, encoded_caption = dataset[0]\n",
    "print(\"📐 Feature vector shape :\", features.shape)\n",
    "print(\"🧾 Caption encodée :\", encoded_caption.tolist())\n",
    "print(\"🧾 Caption décodée :\", tokenizer.decode(encoded_caption.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60917c77",
   "metadata": {},
   "source": [
    "## 🧩 Étape 4 – Collate Function personnalisée\n",
    "\n",
    "| Objectif | Permettre à PyTorch de batcher les légendes de longueur variable en ajoutant du padding |\n",
    "| -------- | --------------------------------------------------------------------------------------- |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccccd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Permet d'empiler un batch de tuples (features, caption) avec padding sur les captions.\n",
    "    \"\"\"\n",
    "    features, captions = zip(*batch)  # batch = liste de tuples\n",
    "\n",
    "    features = torch.stack(features)  # [batch_size, 2048]\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    captions_padded = pad_sequence(captions, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return features, captions_padded, lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c0391",
   "metadata": {},
   "source": [
    "### 📌 Pourquoi c’est important :\n",
    "- Les séquences textuelles (captions) n'ont pas la même longueur.\n",
    "\n",
    "- Le modèle LSTM nécessite des entrées de taille uniforme → `pad_sequence` fait le job.\n",
    "\n",
    "- On garde aussi les longueurs `lengths` pour utiliser `pack_padded_sequence` dans le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b818d",
   "metadata": {},
   "source": [
    "## 📦 Étape 5 – Création des `DataLoaders` entraînement / validation / test\n",
    "| Objectif | Séparer le dataset en trois parties et créer les `DataLoaders` correspondants |\n",
    "| -------- | ----------------------------------------------------------------------------- |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0345c431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Split → Train: 129456 | Val: 16182 | Test: 16182\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# 📏 Proportions des splits\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "total_size = len(dataset)\n",
    "\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size  # le reste\n",
    "\n",
    "# ✂️ Split\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(f\"📊 Split → Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "# 🧪 DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544222a",
   "metadata": {},
   "source": [
    "### 🧠 Remarques\n",
    "- `shuffle=True` pour le training uniquement\n",
    "\n",
    "- `collate_fn` s’applique à tous les DataLoaders\n",
    "\n",
    "- On garde `batch_size=32` mais tu pourras ajuster selon la mémoire GPU dispo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281145e9",
   "metadata": {},
   "source": [
    "## 🧪 Étape 6 – Visualisation d’un batch complet\n",
    "| Objectif | Afficher 5 exemples d’images (features), captions tokenisées et décodées, pour vérification |\n",
    "| -------- | ----------------------------------------------------------------------------- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d1de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch chargé\n",
      "📐 Feature vector shape : torch.Size([64, 2048])\n",
      "🧾 Captions shape : torch.Size([64, 19])\n",
      "\n",
      "🎯 Exemples du batch :\n",
      "\n",
      "🖼️ Exemple 1 :\n",
      "  ➤ Caption encodée : [1, 4, 56, 59, 13, 32, 62, 5, 4, 262, 13, 25, 2]\n",
      "  ➤ Caption décodée : a large group of dogs walking in a body of water\n",
      "\n",
      "🖼️ Exemple 2 :\n",
      "  ➤ Caption encodée : [1, 49, 232, 24, 116, 5, 48, 13, 29, 3, 223, 2]\n",
      "  ➤ Caption décodée : three older people stand in front of an <unk> sign\n",
      "\n",
      "🖼️ Exemple 3 :\n",
      "  ➤ Caption encodée : [1, 4, 20, 5, 4, 297, 136, 3, 378, 2]\n",
      "  ➤ Caption décodée : a girl in a forest carrying <unk> gear\n",
      "\n",
      "🖼️ Exemple 4 :\n",
      "  ➤ Caption encodée : [1, 4, 12, 22, 4, 31, 38, 972, 7, 4, 222, 257, 2]\n",
      "  ➤ Caption décodée : a man wearing a blue shirt crouches on a rocky cliff\n",
      "\n",
      "🖼️ Exemple 5 :\n",
      "  ➤ Caption encodée : [1, 4, 27, 20, 5, 4, 91, 171, 11, 172, 5, 61, 173, 2]\n",
      "  ➤ Caption décodée : a young girl in a pink dress with something in her hand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 🔁 Récupération d’un batch du DataLoader\n",
    "features_batch, captions_batch, lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"✅ Batch chargé\")\n",
    "print(\"📐 Feature vector shape :\", features_batch.shape)   # [batch_size, 2048]\n",
    "print(\"🧾 Captions shape :\", captions_batch.shape)         # [batch_size, max_len]\n",
    "\n",
    "# 🔍 Affichage de quelques exemples\n",
    "print(\"\\n🎯 Exemples du batch :\\n\")\n",
    "for i in range(5):\n",
    "    encoded_caption = captions_batch[i][:lengths[i]].tolist()  # on découpe selon la vraie longueur\n",
    "    decoded_caption = tokenizer.decode(encoded_caption)\n",
    "\n",
    "    print(f\"🖼️ Exemple {i+1} :\")\n",
    "    print(\"  ➤ Caption encodée :\", encoded_caption)\n",
    "    print(\"  ➤ Caption décodée :\", decoded_caption)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392baab",
   "metadata": {},
   "source": [
    "### 🧠 À vérifier\n",
    "- Les captions ne doivent pas commencer/finir par `<unk>` (on les a corrigées précédemment).\n",
    "\n",
    "- Les encodages doivent bien commencer par `<start>` et finir par `<end>` (IDs = 1 et 2).\n",
    "\n",
    "- Vérifie que le padding n’est pas inclus dans `lengths[i]` (c’est bien le cas ici)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3354f2",
   "metadata": {},
   "source": [
    "## ✅ Étape 7 – Vérification des dimensions et métadonnées\n",
    "\n",
    "| Objectif | Valider les tailles du vocabulaire, batch, longueur max, etc. avant modélisation |\n",
    "| -------- | ----------------------------------------------------------------------------- |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab794e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Taille du vocabulaire : 1204\n",
      "📐 Taille des features (par image) : 2048\n",
      "📏 Longueur maximale des captions : 19\n",
      "🧮 Taille du batch : 64\n",
      "🔍 Exemple de token 1 (id): <start>\n",
      "🔍 Exemple de token 2 (id): <end>\n"
     ]
    }
   ],
   "source": [
    "# 📏 Vérification des dimensions\n",
    "print(\"🔢 Taille du vocabulaire :\", tokenizer.vocab_size)\n",
    "print(\"📐 Taille des features (par image) :\", features_batch.shape[1])\n",
    "print(\"📏 Longueur maximale des captions :\", captions_batch.shape[1])\n",
    "print(\"🧮 Taille du batch :\", features_batch.shape[0])\n",
    "\n",
    "# 🧾 Vérif rapide d’un vocab token\n",
    "print(\"🔍 Exemple de token 1 (id):\", tokenizer.idx2word.get(1))\n",
    "print(\"🔍 Exemple de token 2 (id):\", tokenizer.idx2word.get(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff755f23",
   "metadata": {},
   "source": [
    "### ✅ À ce stade, on a terminé la partie préparation des données et loaders :\n",
    "On doit maintenant avoir :\n",
    "\n",
    "- `train_loader`, `val_loader`, `test_loader` fonctionnels 🎯\n",
    "\n",
    "- Un vocab propre, nettoyé et indexé 🧠\n",
    "\n",
    "- Des captions bien formatées : `<start> ... <end>`\n",
    "\n",
    "- Et des features d’images prêtes à être injectées dans un modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e632b2",
   "metadata": {},
   "source": [
    "## | Étape 4 | 🧠 Définition du modèle CNN → LSTM avec Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aaee4f",
   "metadata": {},
   "source": [
    "| Module                 | Description                                                                                 |\n",
    "| ---------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| `EncoderCNN`           | Projette les features extraits (2048-dim) dans un espace de plus petite dimension (ex: 256) |\n",
    "| `Attention`            | Calcule les poids d’attention pour chaque position du vecteur d’image                       |\n",
    "| `DecoderWithAttention` | LSTM conditionné à l’attention + embeddings + prédiction du token suivant                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a5dec",
   "metadata": {},
   "source": [
    "### 🧠 Étape 4.1 – `Attention` : Mécanisme d’attention\n",
    "**Objectif** :\n",
    "L'attention permet au modèle de **focaliser sur différentes parties des features** à chaque étape de génération de mot.\n",
    "Même si nos features sont globales (pas spatiales ici), on ajoute une attention **basée sur le contexte du LSTM** (Bahdanau-style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a043b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_out: [batch_size, encoder_dim]     → features encodées\n",
    "        decoder_hidden: [batch_size, decoder_dim]  → état caché courant du LSTM\n",
    "\n",
    "        returns:\n",
    "            - attention_weighted_encoding: [batch_size, encoder_dim]\n",
    "            - alpha: [batch_size, 1]\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)             # [batch_size, attention_dim]\n",
    "        att2 = self.decoder_att(decoder_hidden)          # [batch_size, attention_dim]\n",
    "        att = self.full_att(self.relu(att1 + att2))      # [batch_size, 1]\n",
    "        alpha = self.softmax(att)                        # [batch_size, 1]\n",
    "        attention_weighted_encoding = encoder_out * alpha  # [batch_size, encoder_dim]\n",
    "        return attention_weighted_encoding, alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627cf897",
   "metadata": {},
   "source": [
    "### 🧠 Étape 4.2 – `DecoderWithAttention` (LSTM + Attention)\n",
    "\n",
    "**Objectif** :\n",
    "Utiliser un LSTM pour générer des mots un par un, avec un mécanisme d’attention à chaque étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f307baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialisation des poids\"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"Initialise h et c à partir des features encodées\"\"\"\n",
    "        h = self.init_h(encoder_out)  # [batch_size, decoder_dim]\n",
    "        c = self.init_c(encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        encoder_out: [batch_size, encoder_dim]           → Features extraites\n",
    "        encoded_captions: [batch_size, max_len]          → Captions target\n",
    "        caption_lengths: [batch_size] ou [batch_size, 1] → Longueur réelle\n",
    "\n",
    "        returns:\n",
    "            - prédictions (logits)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        # 🔐 Sécurité : conversion caption_lengths en Tensor si nécessaire\n",
    "        if isinstance(caption_lengths, list):\n",
    "            caption_lengths = torch.tensor(caption_lengths, dtype=torch.long, device=encoder_out.device)\n",
    "        if caption_lengths.dim() == 2:  # Si [batch_size, 1], on squeeze\n",
    "            caption_lengths = caption_lengths.squeeze(1)\n",
    "\n",
    "        embeddings = self.embedding(encoded_captions)  # [batch_size, max_len, embed_dim]\n",
    "        h, c = self.init_hidden_state(encoder_out)     # h, c : [batch_size, decoder_dim]\n",
    "\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(encoder_out.device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attn_weighted_encoding, _ = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "\n",
    "            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], attn_weighted_encoding], dim=1)\n",
    "            h, c = self.decode_step(input_lstm, (h[:batch_size_t], c[:batch_size_t]))  # LSTMCell\n",
    "\n",
    "            preds = self.fc(self.dropout(h))  # [batch_size_t, vocab_size]\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48cebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📐 Feature shape : torch.Size([64, 2048])\n",
      "🧾 Captions shape : torch.Size([64, 25])\n",
      "✅ Logits shape : torch.Size([64, 24, 1204])\n",
      "📊 BLEU-3 : 0.0024 | ROUGE-L : 0.0081\n"
     ]
    }
   ],
   "source": [
    "# 📦 Imports nécessaires\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ⚙️ BLEU-3 (trigrammes)\n",
    "def compute_bleu(references, hypotheses, n=3):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_tokens = ref.lower().split()\n",
    "        hyp_tokens = hyp.lower().split()\n",
    "        weights = tuple((1. / n for _ in range(n)))\n",
    "        score = sentence_bleu([ref_tokens], hyp_tokens, weights=weights, smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# ⚙️ ROUGE-L\n",
    "def compute_rouge(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        score = scorer.score(ref, hyp)['rougeL'].fmeasure\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# 🧠 Initialisation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decoder = DecoderWithAttention(\n",
    "    attention_dim=256,\n",
    "    embed_dim=256,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# 🧪 Test sur un batch\n",
    "features, captions, lengths = next(iter(train_loader))\n",
    "features, captions = features.to(device), captions.to(device)\n",
    "\n",
    "print(\"📐 Feature shape :\", features.shape)\n",
    "print(\"🧾 Captions shape :\", captions.shape)\n",
    "\n",
    "# 🔁 Forward (pas d'encoder, on a déjà les features)\n",
    "outputs = decoder(features, captions, lengths)  # [B, max_len, vocab_size]\n",
    "print(\"✅ Logits shape :\", outputs.shape)\n",
    "\n",
    "# 🎯 Calcul de BLEU/ROUGE sur 1 batch pour debug\n",
    "predictions = outputs.argmax(-1).detach().cpu().tolist()\n",
    "references = captions[:, 1:].detach().cpu().tolist()  # on ignore <start>\n",
    "\n",
    "decoded_preds = [tokenizer.decode(p) for p in predictions]\n",
    "decoded_refs = [tokenizer.decode(r) for r in references]\n",
    "\n",
    "bleu = compute_bleu(decoded_refs, decoded_preds, n=3)\n",
    "rouge = compute_rouge(decoded_refs, decoded_preds)\n",
    "\n",
    "print(f\"📊 BLEU-3 : {bleu:.4f} | ROUGE-L : {rouge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba9669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_model(decoder, train_loader, val_loader, tokenizer, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    decoder.to(device)\n",
    "\n",
    "    # 📁 Dossier de sortie daté\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = f\"outputs/{timestamp}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(save_dir, \"decoder.pt\")\n",
    "    checkpoint_path = os.path.join(save_dir, \"checkpoint.pt\")\n",
    "    metrics_path = os.path.join(save_dir, \"metrics.json\")\n",
    "    plot_path = os.path.join(save_dir, \"plot.png\")\n",
    "\n",
    "    train_losses, val_bleus, val_rouges = [], [], []\n",
    "    best_bleu = 0\n",
    "    patience_counter = 0\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)\n",
    "\n",
    "    print(f\"🚀 Training started on {device}\")\n",
    "    epoch_bar = tqdm(range(num_epochs), desc=\"📆 Epochs\")\n",
    "\n",
    "    for epoch in epoch_bar:\n",
    "        decoder.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        batch_bar = tqdm(train_loader, desc=f\"🧠 Training Epoch {epoch+1}\", leave=True, position=1)\n",
    "        for features, captions, lengths in batch_bar:\n",
    "            features, captions = features.to(device), captions.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            targets = captions[:, 1:]\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # 🔍 Évaluation\n",
    "        decoder.eval()\n",
    "        all_preds, all_refs = [], []\n",
    "\n",
    "        val_bar = tqdm(val_loader, desc=f\"🔍 Evaluating Epoch {epoch+1}\", leave=True, position=2)\n",
    "        with torch.no_grad():\n",
    "            for features, captions, lengths in val_bar:\n",
    "                features, captions = features.to(device), captions.to(device)\n",
    "                outputs = decoder(features, captions, lengths)\n",
    "                preds = outputs.argmax(-1).detach().cpu().tolist()\n",
    "                refs = captions[:, 1:].detach().cpu().tolist()\n",
    "\n",
    "                decoded_preds = [tokenizer.decode(p) for p in preds]\n",
    "                decoded_refs = [tokenizer.decode(r) for r in refs]\n",
    "                all_preds.extend(decoded_preds)\n",
    "                all_refs.extend(decoded_refs)\n",
    "\n",
    "        bleu = compute_bleu(all_refs, all_preds, n=3)\n",
    "        rouge = compute_rouge(all_refs, all_preds)\n",
    "\n",
    "        val_bleus.append(bleu)\n",
    "        val_rouges.append(rouge)\n",
    "        scheduler.step(bleu)\n",
    "\n",
    "        # ⏹️ Résumé console\n",
    "        print(f\"\\n📊 Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f} | BLEU-3: {bleu:.4f} | ROUGE-L: {rouge:.4f}\\n\")\n",
    "\n",
    "        # 💾 Sauvegarde du modèle\n",
    "        if bleu > best_bleu:\n",
    "            best_bleu = bleu\n",
    "            patience_counter = 0\n",
    "            torch.save(decoder.state_dict(), best_model_path)\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"decoder_state_dict\": decoder.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_losses\": train_losses,\n",
    "                \"val_bleus\": val_bleus,\n",
    "                \"val_rouges\": val_rouges,\n",
    "                \"tokenizer\": tokenizer.word2idx\n",
    "            }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"⛔ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # 💾 Sauvegarde des métriques\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_bleu\": val_bleus,\n",
    "            \"val_rouge\": val_rouges\n",
    "        }, f)\n",
    "\n",
    "    # 📊 Graphe matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_bleus, label=\"BLEU-3\")\n",
    "    plt.plot(val_rouges, label=\"ROUGE-L\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    return train_losses, val_bleus, val_rouges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5234fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(train_losses, val_bleus, val_rouges):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(16, 5))\n",
    "\n",
    "    # 📉 Train Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.title(\"🔧 Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 🎯 BLEU-3\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, val_bleus, label=\"BLEU-3\", color=\"green\", marker='o')\n",
    "    plt.title(\"📊 BLEU-3 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"BLEU-3\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 📝 ROUGE-L\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, val_rouges, label=\"ROUGE-L\", color=\"orange\", marker='o')\n",
    "    plt.title(\"📘 ROUGE-L Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ROUGE-L\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26711e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training started on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📆 Epochs:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "🧠 Training Epoch 1:   0%|          | 0/2023 [00:00<?, ?it/s]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 0/2023 [00:00<?, ?it/s, loss=7.12]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 1/2023 [00:00<14:11,  2.37it/s, loss=7.12]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 1/2023 [00:00<14:11,  2.37it/s, loss=6.94]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 2/2023 [00:00<12:36,  2.67it/s, loss=6.94]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 2/2023 [00:01<12:36,  2.67it/s, loss=6.79]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 3/2023 [00:01<12:07,  2.78it/s, loss=6.79]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 3/2023 [00:01<12:07,  2.78it/s, loss=6.66]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 4/2023 [00:01<11:58,  2.81it/s, loss=6.66]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 4/2023 [00:01<11:58,  2.81it/s, loss=6.46]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 5/2023 [00:01<11:59,  2.80it/s, loss=6.46]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 5/2023 [00:02<11:59,  2.80it/s, loss=6.42]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 6/2023 [00:02<12:15,  2.74it/s, loss=6.42]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 6/2023 [00:02<12:15,  2.74it/s, loss=6.35]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 7/2023 [00:02<12:13,  2.75it/s, loss=6.35]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 7/2023 [00:02<12:13,  2.75it/s, loss=6.17]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 8/2023 [00:02<12:12,  2.75it/s, loss=6.17]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 8/2023 [00:03<12:12,  2.75it/s, loss=6.26]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 9/2023 [00:03<12:08,  2.77it/s, loss=6.26]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 9/2023 [00:03<12:08,  2.77it/s, loss=6.13]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 10/2023 [00:03<11:48,  2.84it/s, loss=6.13]\u001b[A\n",
      "🧠 Training Epoch 1:   0%|          | 10/2023 [00:03<11:48,  2.84it/s, loss=6.15]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 11/2023 [00:03<11:37,  2.89it/s, loss=6.15]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 11/2023 [00:04<11:37,  2.89it/s, loss=6.03]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 12/2023 [00:04<11:13,  2.99it/s, loss=6.03]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 12/2023 [00:04<11:13,  2.99it/s, loss=6.03]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 13/2023 [00:04<11:37,  2.88it/s, loss=6.03]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 13/2023 [00:04<11:37,  2.88it/s, loss=5.99]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 14/2023 [00:04<11:31,  2.90it/s, loss=5.99]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 14/2023 [00:05<11:31,  2.90it/s, loss=5.96]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 15/2023 [00:05<11:28,  2.92it/s, loss=5.96]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 15/2023 [00:05<11:28,  2.92it/s, loss=5.99]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 16/2023 [00:05<11:44,  2.85it/s, loss=5.99]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 16/2023 [00:06<11:44,  2.85it/s, loss=5.82]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 17/2023 [00:06<12:00,  2.78it/s, loss=5.82]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 17/2023 [00:06<12:00,  2.78it/s, loss=5.99]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 18/2023 [00:06<12:06,  2.76it/s, loss=5.99]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 18/2023 [00:06<12:06,  2.76it/s, loss=5.77]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 19/2023 [00:06<12:39,  2.64it/s, loss=5.77]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 19/2023 [00:07<12:39,  2.64it/s, loss=5.85]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 20/2023 [00:07<12:42,  2.63it/s, loss=5.85]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 20/2023 [00:07<12:42,  2.63it/s, loss=5.86]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 21/2023 [00:07<12:38,  2.64it/s, loss=5.86]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 21/2023 [00:07<12:38,  2.64it/s, loss=5.7] \u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 22/2023 [00:07<12:26,  2.68it/s, loss=5.7]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 22/2023 [00:08<12:26,  2.68it/s, loss=5.67]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 23/2023 [00:08<12:01,  2.77it/s, loss=5.67]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 23/2023 [00:08<12:01,  2.77it/s, loss=5.82]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 24/2023 [00:08<11:42,  2.84it/s, loss=5.82]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 24/2023 [00:08<11:42,  2.84it/s, loss=5.6] \u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 25/2023 [00:08<11:39,  2.86it/s, loss=5.6]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|          | 25/2023 [00:09<11:39,  2.86it/s, loss=5.59]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 26/2023 [00:09<11:25,  2.91it/s, loss=5.59]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 26/2023 [00:09<11:25,  2.91it/s, loss=5.63]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 27/2023 [00:09<11:27,  2.90it/s, loss=5.63]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 27/2023 [00:09<11:27,  2.90it/s, loss=5.51]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 28/2023 [00:09<11:23,  2.92it/s, loss=5.51]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 28/2023 [00:10<11:23,  2.92it/s, loss=5.74]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 29/2023 [00:10<11:18,  2.94it/s, loss=5.74]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 29/2023 [00:10<11:18,  2.94it/s, loss=5.42]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 30/2023 [00:10<10:43,  3.10it/s, loss=5.42]\u001b[A\n",
      "🧠 Training Epoch 1:   1%|▏         | 30/2023 [00:10<10:43,  3.10it/s, loss=5.6] \u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 31/2023 [00:10<10:44,  3.09it/s, loss=5.6]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 31/2023 [00:11<10:44,  3.09it/s, loss=5.72]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 32/2023 [00:11<10:45,  3.08it/s, loss=5.72]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 32/2023 [00:11<10:45,  3.08it/s, loss=5.64]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 33/2023 [00:11<11:08,  2.98it/s, loss=5.64]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 33/2023 [00:11<11:08,  2.98it/s, loss=5.5] \u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 34/2023 [00:11<11:05,  2.99it/s, loss=5.5]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 34/2023 [00:12<11:05,  2.99it/s, loss=5.54]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 35/2023 [00:12<10:59,  3.02it/s, loss=5.54]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 35/2023 [00:12<10:59,  3.02it/s, loss=5.45]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 36/2023 [00:12<11:16,  2.94it/s, loss=5.45]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 36/2023 [00:12<11:16,  2.94it/s, loss=5.53]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 37/2023 [00:12<11:10,  2.96it/s, loss=5.53]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 37/2023 [00:13<11:10,  2.96it/s, loss=5.51]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 38/2023 [00:13<11:31,  2.87it/s, loss=5.51]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 38/2023 [00:13<11:31,  2.87it/s, loss=5.55]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 39/2023 [00:13<11:25,  2.89it/s, loss=5.55]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 39/2023 [00:14<11:25,  2.89it/s, loss=5.43]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 40/2023 [00:14<11:25,  2.89it/s, loss=5.43]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 40/2023 [00:14<11:25,  2.89it/s, loss=5.52]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 41/2023 [00:14<11:21,  2.91it/s, loss=5.52]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 41/2023 [00:14<11:21,  2.91it/s, loss=5.54]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 42/2023 [00:14<11:47,  2.80it/s, loss=5.54]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 42/2023 [00:15<11:47,  2.80it/s, loss=5.54]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 43/2023 [00:15<11:56,  2.76it/s, loss=5.54]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 43/2023 [00:15<11:56,  2.76it/s, loss=5.46]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 44/2023 [00:15<11:58,  2.75it/s, loss=5.46]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 44/2023 [00:15<11:58,  2.75it/s, loss=5.35]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 45/2023 [00:15<11:42,  2.82it/s, loss=5.35]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 45/2023 [00:16<11:42,  2.82it/s, loss=5.57]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 46/2023 [00:16<11:48,  2.79it/s, loss=5.57]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 46/2023 [00:16<11:48,  2.79it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 47/2023 [00:16<11:43,  2.81it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 47/2023 [00:16<11:43,  2.81it/s, loss=5.33]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 48/2023 [00:16<11:34,  2.84it/s, loss=5.33]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 48/2023 [00:17<11:34,  2.84it/s, loss=5.35]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 49/2023 [00:17<11:15,  2.92it/s, loss=5.35]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 49/2023 [00:17<11:15,  2.92it/s, loss=5.44]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 50/2023 [00:17<11:17,  2.91it/s, loss=5.44]\u001b[A\n",
      "🧠 Training Epoch 1:   2%|▏         | 50/2023 [00:17<11:17,  2.91it/s, loss=5.5] \u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 51/2023 [00:17<11:26,  2.87it/s, loss=5.5]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 51/2023 [00:18<11:26,  2.87it/s, loss=5.53]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 52/2023 [00:18<11:10,  2.94it/s, loss=5.53]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 52/2023 [00:18<11:10,  2.94it/s, loss=5.48]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 53/2023 [00:18<11:09,  2.94it/s, loss=5.48]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 53/2023 [00:18<11:09,  2.94it/s, loss=5.36]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 54/2023 [00:18<11:16,  2.91it/s, loss=5.36]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 54/2023 [00:19<11:16,  2.91it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 55/2023 [00:19<11:25,  2.87it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 55/2023 [00:19<11:25,  2.87it/s, loss=5.47]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 56/2023 [00:19<11:34,  2.83it/s, loss=5.47]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 56/2023 [00:19<11:34,  2.83it/s, loss=5.46]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 57/2023 [00:19<11:20,  2.89it/s, loss=5.46]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 57/2023 [00:20<11:20,  2.89it/s, loss=5.26]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 58/2023 [00:20<11:18,  2.90it/s, loss=5.26]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 58/2023 [00:20<11:18,  2.90it/s, loss=5.32]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 59/2023 [00:20<11:25,  2.87it/s, loss=5.32]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 59/2023 [00:20<11:25,  2.87it/s, loss=5.41]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 60/2023 [00:21<11:15,  2.90it/s, loss=5.41]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 60/2023 [00:21<11:15,  2.90it/s, loss=5.47]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 61/2023 [00:21<11:19,  2.89it/s, loss=5.47]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 61/2023 [00:21<11:19,  2.89it/s, loss=5.37]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 62/2023 [00:21<11:05,  2.95it/s, loss=5.37]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 62/2023 [00:22<11:05,  2.95it/s, loss=5.5] \u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 63/2023 [00:22<11:22,  2.87it/s, loss=5.5]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 63/2023 [00:22<11:22,  2.87it/s, loss=5.42]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 64/2023 [00:22<11:43,  2.78it/s, loss=5.42]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 64/2023 [00:22<11:43,  2.78it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 65/2023 [00:22<11:38,  2.80it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 65/2023 [00:23<11:38,  2.80it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 66/2023 [00:23<11:35,  2.81it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 66/2023 [00:23<11:35,  2.81it/s, loss=5.34]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 67/2023 [00:23<11:37,  2.80it/s, loss=5.34]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 67/2023 [00:23<11:37,  2.80it/s, loss=5.37]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 68/2023 [00:23<11:14,  2.90it/s, loss=5.37]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 68/2023 [00:24<11:14,  2.90it/s, loss=5.27]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 69/2023 [00:24<11:10,  2.91it/s, loss=5.27]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 69/2023 [00:24<11:10,  2.91it/s, loss=5.47]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 70/2023 [00:24<11:14,  2.90it/s, loss=5.47]\u001b[A\n",
      "🧠 Training Epoch 1:   3%|▎         | 70/2023 [00:24<11:14,  2.90it/s, loss=5.28]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 71/2023 [00:24<11:13,  2.90it/s, loss=5.28]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 71/2023 [00:25<11:13,  2.90it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 72/2023 [00:25<11:05,  2.93it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 72/2023 [00:25<11:05,  2.93it/s, loss=5.26]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 73/2023 [00:25<10:56,  2.97it/s, loss=5.26]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 73/2023 [00:25<10:56,  2.97it/s, loss=5.42]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 74/2023 [00:25<11:37,  2.80it/s, loss=5.42]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 74/2023 [00:26<11:37,  2.80it/s, loss=5.25]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 75/2023 [00:26<11:41,  2.78it/s, loss=5.25]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▎         | 75/2023 [00:26<11:41,  2.78it/s, loss=5.24]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 76/2023 [00:26<11:52,  2.73it/s, loss=5.24]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 76/2023 [00:27<11:52,  2.73it/s, loss=5.32]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 77/2023 [00:27<12:00,  2.70it/s, loss=5.32]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 77/2023 [00:27<12:00,  2.70it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 78/2023 [00:27<12:18,  2.63it/s, loss=5.31]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 78/2023 [00:27<12:18,  2.63it/s, loss=5.46]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 79/2023 [00:27<12:12,  2.65it/s, loss=5.46]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 79/2023 [00:28<12:12,  2.65it/s, loss=5.26]\u001b[A\n",
      "🧠 Training Epoch 1:   4%|▍         | 80/2023 [00:28<11:31,  2.81it/s, loss=5.26]\u001b[A\n",
      "📆 Epochs:   0%|          | 0/5 [00:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 🚀 Lancement de l'entraînement\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m train_losses, val_bleus, val_rouges \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# ← tu peux augmenter\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# ← early stopping si pas d’amélioration BLEU\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(decoder, train_loader, val_loader, tokenizer, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[1;32m     32\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     34\u001b[0m batch_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🧠 Training Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, captions, lengths \u001b[38;5;129;01min\u001b[39;00m batch_bar:\n\u001b[1;32m     36\u001b[0m     features, captions \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device), captions\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mImageCaptionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 📦 Chargement des features\u001b[39;00m\n\u001b[1;32m     25\u001b[0m feature_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 🔡 Encodage de la légende\u001b[39;00m\n\u001b[1;32m     29\u001b[0m caption \u001b[38;5;241m=\u001b[39m clean_caption(caption) \n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 🔧 Paramètres du modèle\n",
    "embed_dim = 256\n",
    "attention_dim = 256\n",
    "decoder_dim = 512\n",
    "vocab_size = tokenizer.vocab_size\n",
    "dropout = 0.5\n",
    "\n",
    "# 🧠 Initialisation du modèle\n",
    "decoder = DecoderWithAttention(\n",
    "    attention_dim=attention_dim,\n",
    "    embed_dim=embed_dim,\n",
    "    decoder_dim=decoder_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=2048,  # ← dépend du backbone utilisé en amont\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# ⚙️ Optimiseur et critère de perte\n",
    "params = list(decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# 🚀 Lancement de l'entraînement\n",
    "train_losses, val_bleus, val_rouges = train_model(\n",
    "    decoder=decoder,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=5,      # ← tu peux augmenter\n",
    "    patience=3          # ← early stopping si pas d’amélioration BLEU\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
