{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50063962",
   "metadata": {},
   "source": [
    "# 📘 Plan du Notebook - `04_dataset_and_dataloader.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710afe9",
   "metadata": {},
   "source": [
    "| Étape | Objectif                                                                                  |\n",
    "| ----- | ----------------------------------------------------------------------------------------- |\n",
    "| 1     | 📁 Chargement des features `.pt` et des captions alignées (`.token.txt`)                  |\n",
    "| 2     | 🔄 Construction de la liste enrichie `(feature_path, caption)`                            |\n",
    "| 3     | 🧱 Définition d’une classe `ImageCaptionDataset` (hérite de `torch.utils.data.Dataset`)   |\n",
    "| 4     | 🧩 Création d’un `collate_fn` personnalisé (padding dynamique des captions)               |\n",
    "| 5     | 📦 Création des `DataLoader` entraînement/test/val (avec `torch.utils.data.random_split`) |\n",
    "| 6     | 🧪 Visualisation d’un batch (image\\_id, légende tokenisée, shape tensors)                 |\n",
    "| 7     | ✅ Vérification des dimensions, vocab size, etc.                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95c48f",
   "metadata": {},
   "source": [
    "### 🧠 Remarques intégrées :\n",
    "- ✅ Toutes les captions (5 par image) sont associées à toutes les images (originale + augmentée).\n",
    "\n",
    "- ✅ Cela crée plus d’échantillons tout en gardant le dataset simple à manipuler.\n",
    "\n",
    "- ✅ Le `collate_fn` utilisera le tokenizer (chargé depuis `tokenizer.pkl`) pour encoder + padd les captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e5a484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Nombre de features chargées : 32364\n",
      "✅ Captions alignées pour 8091 images\n"
     ]
    }
   ],
   "source": [
    "# 📦 Imports\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# 📁 Dossiers\n",
    "features_dir = Path(\"../data/processed/features_resnet_global\")\n",
    "captions_file = \"../data/raw/Flickr8k_text/Flickr8k.token.txt\"\n",
    "tokenizer_path = \"../data/vocab/tokenizer.pkl\"\n",
    "\n",
    "# ✅ Chargement des features disponibles\n",
    "feature_files = list(features_dir.glob(\"*.pt\"))\n",
    "feature_ids = [f.stem.split(\"_aug\")[0] for f in feature_files]\n",
    "\n",
    "print(f\"🧠 Nombre de features chargées : {len(feature_files)}\")\n",
    "\n",
    "# 📖 Construction du dictionnaire {image_id: [captions]}\n",
    "captions_dict = defaultdict(list)\n",
    "with open(captions_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            image_tag, caption = line.strip().split(\"\\t\")\n",
    "            image_id = image_tag.split(\"#\")[0].split(\".\")[0]\n",
    "            if image_id in feature_ids:\n",
    "                captions_dict[image_id].append(caption.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"⛔ Ligne corrompue : {line}\")\n",
    "\n",
    "print(f\"✅ Captions alignées pour {len(captions_dict)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc545d0",
   "metadata": {},
   "source": [
    "## 🧩 Étape 2 – Construction des paires (features, caption)\n",
    "\n",
    "### 🎯 Objectif :\n",
    "Associer chaque feature `.pt` à **une seule caption** (aléatoire parmi les 5 possibles), pour former un dataset `(feature_path, caption)` utilisable dans le `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6aff8",
   "metadata": {},
   "source": [
    "### 🔧 Pourquoi ce choix :\n",
    "- Chaque image (et ses versions augmentées) aura **1 seule caption associée** pour éviter les doublons (sinon on duplique inutilement les features).\n",
    "\n",
    "- On pourra éventuellement en faire plus tard (ex: `dataset.expand_with_5_captions()`), mais une seule suffit pour démarrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7576781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paires (feature, caption) construites : 32364\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 📄 Création de la liste (feature_path, caption)\n",
    "dataset_pairs = []\n",
    "\n",
    "for feature_file in feature_files:\n",
    "    image_id = feature_file.stem.split(\"_aug\")[0]\n",
    "    \n",
    "    if image_id in captions_dict:\n",
    "        # 🎯 Caption aléatoire parmi les 5 possibles\n",
    "        caption = random.choice(captions_dict[image_id])\n",
    "        dataset_pairs.append((feature_file, caption))\n",
    "\n",
    "print(f\"✅ Paires (feature, caption) construites : {len(dataset_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac167d",
   "metadata": {},
   "source": [
    "## 🧠 Étape 3 – Classe `ImageCaptionDataset`\n",
    "\n",
    "### 🎯 Objectif :\n",
    "Créer une classe héritant de `torch.utils.data.Dataset` pour :\n",
    "\n",
    "- charger les features `.pt` (déjà extraites),\n",
    "\n",
    "- encoder la caption associée (via ton tokenizer),\n",
    "\n",
    "- retourner une paire `(features_tensor, encoded_caption_tensor)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33003177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, features_dir, captions_dict, tokenizer, max_length=30):\n",
    "        self.features_dir = Path(features_dir)\n",
    "        self.captions = captions_dict  # dict[image_id] = caption\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_ids = list(self.captions.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        feature_path = self.features_dir / f\"{image_id}.pt\"\n",
    "        features = torch.load(feature_path)\n",
    "\n",
    "        caption = self.captions[image_id]\n",
    "\n",
    "        # ✨ Nettoyage de la légende\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z ]\", \"\", caption)\n",
    "\n",
    "        # ✨ Encodage\n",
    "        encoded = self.tokenizer.encode(caption)\n",
    "        encoded = encoded[:self.max_length]  # 🔪 Troncature si trop long\n",
    "\n",
    "        return features, torch.tensor(encoded, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c2729",
   "metadata": {},
   "source": [
    "### 📌 Remarques :\n",
    "- On ajoute un `max_len=37` par défaut (valeur obtenue lors de l'utilisation `03_vocab_building.ipynb`).\n",
    "\n",
    "- Le `padding` à droite permet une gestion plus simple dans le `collate_fn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fbca1",
   "metadata": {},
   "source": [
    "## 🧱 Étape 4 – Fonction `collate_fn` personnalisée\n",
    "\n",
    "### 🎯 Objectif :\n",
    "Créer une fonction `collate_fn` qui sera utilisée par le `DataLoader` pour :\n",
    "\n",
    "- empiler proprement les features (`[batch_size, 2048]`)\n",
    "\n",
    "- empiler les séquences texte déjà padées (`[batch_size, max_len]`)\n",
    "\n",
    "- retourner un batch `(X, y)` prêt à être traité par le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759b1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batch = liste de tuples : (features, encoded_caption)\n",
    "    \"\"\"\n",
    "    features_batch = torch.stack([item[0] for item in batch])          # (B, 2048)\n",
    "    captions_batch = torch.stack([item[1] for item in batch])          # (B, max_len)\n",
    "\n",
    "    return features_batch, captions_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675531b1",
   "metadata": {},
   "source": [
    "### 🧾 Étape 5 – Construction du `Dataset` et du `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4f7c6",
   "metadata": {},
   "source": [
    "#### 📦 1. Nouveau `Dataset` `ImageCaptionDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817afcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    # On garde les apostrophes dans les contractions (it's, don't)\n",
    "    caption = re.sub(r\"[^a-zA-Z0-9'\\s]\", \"\", caption)  # supprime tout sauf lettres, chiffres, apostrophes, espaces\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption)\n",
    "    return caption.strip()\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, features_dir, captions_dict, tokenizer, max_length=37):\n",
    "        self.features_dir = Path(features_dir)\n",
    "        self.captions_dict = captions_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.samples = []\n",
    "        for image_id, captions in captions_dict.items():\n",
    "            for caption in captions:\n",
    "                self.samples.append((image_id, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, caption = self.samples[idx]\n",
    "\n",
    "        # Nettoyage de la légende avant encodage\n",
    "        caption = clean_caption(caption)\n",
    "\n",
    "        # Chargement des features (Tensor de taille [2048])\n",
    "        feature_path = self.features_dir / f\"{image_id}.pt\"\n",
    "        features = torch.load(feature_path)\n",
    "\n",
    "        # Encodage de la légende\n",
    "        encoded = self.tokenizer.encode(caption)\n",
    "        encoded = encoded[:self.max_length]  # 🔪 Troncature si trop long\n",
    "\n",
    "        return features, torch.tensor(encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fd928",
   "metadata": {},
   "source": [
    "#### 🔧 2. `collate_fn` pour padding dynamique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4564325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    features, captions = zip(*batch)\n",
    "\n",
    "    # Stack les features [batch, 2048]\n",
    "    features = torch.stack(features)\n",
    "\n",
    "    # Padding des séquences\n",
    "    lengths = [len(c) for c in captions]\n",
    "    max_len = max(lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "\n",
    "    return features, padded_captions, torch.tensor(lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6a1b0",
   "metadata": {},
   "source": [
    "#### 🚀 3. Création du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cc463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "captions_file = \"../data/raw/Flickr8k_text/Flickr8k.token.txt\"\n",
    "features_dir = Path(\"../data/processed/features_resnet_global\")\n",
    "\n",
    "extracted_ids = {f.stem.split(\"_aug\")[0] for f in features_dir.glob(\"*.pt\")}\n",
    "\n",
    "aligned_captions = defaultdict(list)\n",
    "\n",
    "with open(captions_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            image_tag, caption = line.strip().split('\\t')\n",
    "            image_id = image_tag.split('#')[0].split('.')[0]\n",
    "            if image_id in extracted_ids:\n",
    "                aligned_captions[image_id].append(caption.strip())\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b7b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Tokenizer chargé : <class '__main__.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Classe Tokenizer à copier (même que dans 03_)\n",
    "class Tokenizer:\n",
    "    def __init__(self, word2idx):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.start_token = \"<start>\"\n",
    "        self.end_token = \"<end>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.pad_token_id = self.word2idx[self.pad_token]\n",
    "        self.start_token_id = self.word2idx[self.start_token]\n",
    "        self.end_token_id = self.word2idx[self.end_token]\n",
    "        self.unk_token_id = self.word2idx[self.unk_token]\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def encode(self, caption, add_special_tokens=True):\n",
    "        tokens = caption.strip().split()\n",
    "        token_ids = [self.word2idx.get(token, self.unk_token_id) for token in tokens]\n",
    "        if add_special_tokens:\n",
    "            return [self.start_token_id] + token_ids + [self.end_token_id]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids, remove_special_tokens=True):\n",
    "        words = [self.idx2word.get(idx, self.unk_token) for idx in token_ids]\n",
    "        if remove_special_tokens:\n",
    "            words = [w for w in words if w not in [self.pad_token, self.start_token, self.end_token]]\n",
    "        return \" \".join(words)\n",
    "\n",
    "# ✅ Chargement de l'objet\n",
    "import pickle\n",
    "\n",
    "with open(\"../data/vocab/tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(\"🧠 Tokenizer chargé :\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f029fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoader prêt avec 1265 batchs\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = ImageCaptionDataset(\n",
    "    features_dir=\"../data/processed/features_resnet_global\",\n",
    "    captions_dict=aligned_captions,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=37\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"✅ DataLoader prêt avec {len(dataloader)} batchs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad494a9e",
   "metadata": {},
   "source": [
    "#### 👀 4. (Optionnel) Visualisation d’un batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b17714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded : [1, 4, 12, 8, 120, 2]\n",
      "Decoded : a man is climbing\n"
     ]
    }
   ],
   "source": [
    "test_caption = \"a man is climbing\"\n",
    "print(\"Encoded :\", tokenizer.encode(test_caption))\n",
    "print(\"Decoded :\", tokenizer.decode(tokenizer.encode(test_caption)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e3f1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch chargé\n",
      "Features shape : torch.Size([32, 2048])\n",
      "Captions shape : torch.Size([32, 24])\n",
      "\n",
      "🖼️ Extrait 1 :\n",
      "  ➤ Vector shape : torch.Size([2048])\n",
      "  ➤ Caption (decoded) : a white and brown spotted dog runs along the snow to catch a ball\n",
      "\n",
      "🖼️ Extrait 2 :\n",
      "  ➤ Vector shape : torch.Size([2048])\n",
      "  ➤ Caption (decoded) : two small dogs run across the green grass\n",
      "\n",
      "🖼️ Extrait 3 :\n",
      "  ➤ Vector shape : torch.Size([2048])\n",
      "  ➤ Caption (decoded) : a brown dog runs in the grass with one ear up\n",
      "\n",
      "🖼️ Extrait 4 :\n",
      "  ➤ Vector shape : torch.Size([2048])\n",
      "  ➤ Caption (decoded) : a tan dog is standing in front of some plants\n",
      "\n",
      "🖼️ Extrait 5 :\n",
      "  ➤ Vector shape : torch.Size([2048])\n",
      "  ➤ Caption (decoded) : man with no shirt and <unk> on back airborne with skateboard in hand\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# 🧠 Chargement d'un batch\n",
    "features_batch, captions_batch, lengths = next(iter(dataloader))\n",
    "\n",
    "print(\"✅ Batch chargé\")\n",
    "print(\"Features shape :\", features_batch.shape)  # [batch_size, 2048]\n",
    "print(\"Captions shape :\", captions_batch.shape)  # [batch_size, max_seq_len]\n",
    "\n",
    "# 🔁 Affichage aléatoire de 5 exemples\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(captions_batch) - 1)\n",
    "    caption_ids = captions_batch[idx][:lengths[idx]].tolist()\n",
    "    decoded = tokenizer.decode(caption_ids)\n",
    "\n",
    "    print(f\"\\n🖼️ Extrait {i+1} :\")\n",
    "    print(f\"  ➤ Vector shape : {features_batch[idx].shape}\")\n",
    "    print(f\"  ➤ Caption (decoded) : {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f31d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's a beautiful day isn't it\n"
     ]
    }
   ],
   "source": [
    "original = \"It's a beautiful day, isn't it?\"\n",
    "cleaned = clean_caption(original)\n",
    "print(cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
