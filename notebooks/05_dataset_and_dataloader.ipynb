{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50063962",
   "metadata": {},
   "source": [
    "# üìò Plan du Notebook - `04_dataset_and_dataloader.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710afe9",
   "metadata": {},
   "source": [
    "| √âtape | Objectif                                                                                  |\n",
    "| ----- | ----------------------------------------------------------------------------------------- |\n",
    "| 1     | üìÅ Chargement des features `.pt` et des captions align√©es (`.token.txt`)                  |\n",
    "| 2     | üîÑ Construction de la liste enrichie `(feature_path, caption)`                            |\n",
    "| 3     | üß± D√©finition d‚Äôune classe `ImageCaptionDataset` (h√©rite de `torch.utils.data.Dataset`)   |\n",
    "| 4     | üß© Cr√©ation d‚Äôun `collate_fn` personnalis√© (padding dynamique des captions)               |\n",
    "| 5     | üì¶ Cr√©ation des `DataLoader` entra√Ænement/test/val (avec `torch.utils.data.random_split`) |\n",
    "| 6     | üß™ Visualisation d‚Äôun batch (image\\_id, l√©gende tokenis√©e, shape tensors)                 |\n",
    "| 7     | ‚úÖ V√©rification des dimensions, vocab size, etc.                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95c48f",
   "metadata": {},
   "source": [
    "### üß† Remarques int√©gr√©es :\n",
    "- ‚úÖ Toutes les captions (5 par image) sont associ√©es √† toutes les images (originale + augment√©e).\n",
    "\n",
    "- ‚úÖ Cela cr√©e plus d‚Äô√©chantillons tout en gardant le dataset simple √† manipuler.\n",
    "\n",
    "- ‚úÖ Le `collate_fn` utilisera le tokenizer (charg√© depuis `tokenizer.pkl`) pour encoder + padd les captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e5a484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Nombre de features charg√©es : 32364\n",
      "‚úÖ Captions align√©es pour 8091 images\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# üìÅ Dossiers\n",
    "features_dir = Path(\"../data/processed/features_resnet_global\")\n",
    "captions_file = \"../data/raw/Flickr8k_text/Flickr8k.token.txt\"\n",
    "tokenizer_path = \"../data/vocab/tokenizer.pkl\"\n",
    "\n",
    "# ‚úÖ Chargement des features disponibles\n",
    "feature_files = list(features_dir.glob(\"*.pt\"))\n",
    "feature_ids = [f.stem.split(\"_aug\")[0] for f in feature_files]\n",
    "\n",
    "print(f\"üß† Nombre de features charg√©es : {len(feature_files)}\")\n",
    "\n",
    "# üìñ Construction du dictionnaire {image_id: [captions]}\n",
    "captions_dict = defaultdict(list)\n",
    "with open(captions_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            image_tag, caption = line.strip().split(\"\\t\")\n",
    "            image_id = image_tag.split(\"#\")[0].split(\".\")[0]\n",
    "            if image_id in feature_ids:\n",
    "                captions_dict[image_id].append(caption.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"‚õî Ligne corrompue : {line}\")\n",
    "\n",
    "print(f\"‚úÖ Captions align√©es pour {len(captions_dict)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc545d0",
   "metadata": {},
   "source": [
    "## üß© √âtape 2 ‚Äì Construction des paires (features, caption)\n",
    "\n",
    "### üéØ Objectif :\n",
    "Associer chaque feature `.pt` √† **une seule caption** (al√©atoire parmi les 5 possibles), pour former un dataset `(feature_path, caption)` utilisable dans le `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6aff8",
   "metadata": {},
   "source": [
    "### üîß Pourquoi ce choix :\n",
    "- Chaque image (et ses versions augment√©es) aura **1 seule caption associ√©e** pour √©viter les doublons (sinon on duplique inutilement les features).\n",
    "\n",
    "- On pourra √©ventuellement en faire plus tard (ex: `dataset.expand_with_5_captions()`), mais une seule suffit pour d√©marrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7576781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Paires (feature, caption) construites : 32364\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# üìÑ Cr√©ation de la liste (feature_path, caption)\n",
    "dataset_pairs = []\n",
    "\n",
    "for feature_file in feature_files:\n",
    "    image_id = feature_file.stem.split(\"_aug\")[0]\n",
    "    \n",
    "    if image_id in captions_dict:\n",
    "        # üéØ Caption al√©atoire parmi les 5 possibles\n",
    "        caption = random.choice(captions_dict[image_id])\n",
    "        dataset_pairs.append((feature_file, caption))\n",
    "\n",
    "print(f\"‚úÖ Paires (feature, caption) construites : {len(dataset_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac167d",
   "metadata": {},
   "source": [
    "## üß† √âtape 3 ‚Äì Classe `ImageCaptionDataset`\n",
    "\n",
    "### üéØ Objectif :\n",
    "Cr√©er une classe h√©ritant de `torch.utils.data.Dataset` pour :\n",
    "\n",
    "- charger les features `.pt` (d√©j√† extraites),\n",
    "\n",
    "- encoder la caption associ√©e (via ton tokenizer),\n",
    "\n",
    "- retourner une paire `(features_tensor, encoded_caption_tensor)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33003177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, features_dir, captions_dict, tokenizer, max_length=30):\n",
    "        self.features_dir = Path(features_dir)\n",
    "        self.captions = captions_dict  # dict[image_id] = caption\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_ids = list(self.captions.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        feature_path = self.features_dir / f\"{image_id}.pt\"\n",
    "        features = torch.load(feature_path)\n",
    "\n",
    "        caption = self.captions[image_id]\n",
    "\n",
    "        # ‚ú® Nettoyage de la l√©gende\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z ]\", \"\", caption)\n",
    "\n",
    "        # ‚ú® Encodage\n",
    "        encoded = self.tokenizer.encode(caption)\n",
    "        encoded = encoded[:self.max_length]  # üî™ Troncature si trop long\n",
    "\n",
    "        return features, torch.tensor(encoded, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c2729",
   "metadata": {},
   "source": [
    "### üìå Remarques :\n",
    "- On ajoute un `max_len=37` par d√©faut (valeur obtenue lors de l'utilisation `03_vocab_building.ipynb`).\n",
    "\n",
    "- Le `padding` √† droite permet une gestion plus simple dans le `collate_fn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fbca1",
   "metadata": {},
   "source": [
    "## üß± √âtape 4 ‚Äì Fonction `collate_fn` personnalis√©e\n",
    "\n",
    "### üéØ Objectif :\n",
    "Cr√©er une fonction `collate_fn` qui sera utilis√©e par le `DataLoader` pour :\n",
    "\n",
    "- empiler proprement les features (`[batch_size, 2048]`)\n",
    "\n",
    "- empiler les s√©quences texte d√©j√† pad√©es (`[batch_size, max_len]`)\n",
    "\n",
    "- retourner un batch `(X, y)` pr√™t √† √™tre trait√© par le mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759b1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batch = liste de tuples : (features, encoded_caption)\n",
    "    \"\"\"\n",
    "    features_batch = torch.stack([item[0] for item in batch])          # (B, 2048)\n",
    "    captions_batch = torch.stack([item[1] for item in batch])          # (B, max_len)\n",
    "\n",
    "    return features_batch, captions_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675531b1",
   "metadata": {},
   "source": [
    "### üßæ √âtape 5 ‚Äì Construction du `Dataset` et du `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4f7c6",
   "metadata": {},
   "source": [
    "#### üì¶ 1. Nouveau `Dataset` `ImageCaptionDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817afcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    # On garde les apostrophes dans les contractions (it's, don't)\n",
    "    caption = re.sub(r\"[^a-zA-Z0-9'\\s]\", \"\", caption)  # supprime tout sauf lettres, chiffres, apostrophes, espaces\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption)\n",
    "    return caption.strip()\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, features_dir, captions_dict, tokenizer, max_length=37):\n",
    "        self.features_dir = Path(features_dir)\n",
    "        self.captions_dict = captions_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.samples = []\n",
    "        for image_id, captions in captions_dict.items():\n",
    "            for caption in captions:\n",
    "                self.samples.append((image_id, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, caption = self.samples[idx]\n",
    "\n",
    "        # Nettoyage de la l√©gende avant encodage\n",
    "        caption = clean_caption(caption)\n",
    "\n",
    "        # Chargement des features (Tensor de taille [2048])\n",
    "        feature_path = self.features_dir / f\"{image_id}.pt\"\n",
    "        features = torch.load(feature_path)\n",
    "\n",
    "        # Encodage de la l√©gende\n",
    "        encoded = self.tokenizer.encode(caption)\n",
    "        encoded = encoded[:self.max_length]  # üî™ Troncature si trop long\n",
    "\n",
    "        return features, torch.tensor(encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fd928",
   "metadata": {},
   "source": [
    "#### üîß 2. `collate_fn` pour padding dynamique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4564325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    features, captions = zip(*batch)\n",
    "\n",
    "    # Stack les features [batch, 2048]\n",
    "    features = torch.stack(features)\n",
    "\n",
    "    # Padding des s√©quences\n",
    "    lengths = [len(c) for c in captions]\n",
    "    max_len = max(lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "\n",
    "    return features, padded_captions, torch.tensor(lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6a1b0",
   "metadata": {},
   "source": [
    "#### üöÄ 3. Cr√©ation du DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cc463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "captions_file = \"../data/raw/Flickr8k_text/Flickr8k.token.txt\"\n",
    "features_dir = Path(\"../data/processed/features_resnet_global\")\n",
    "\n",
    "extracted_ids = {f.stem.split(\"_aug\")[0] for f in features_dir.glob(\"*.pt\")}\n",
    "\n",
    "aligned_captions = defaultdict(list)\n",
    "\n",
    "with open(captions_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            image_tag, caption = line.strip().split('\\t')\n",
    "            image_id = image_tag.split('#')[0].split('.')[0]\n",
    "            if image_id in extracted_ids:\n",
    "                aligned_captions[image_id].append(caption.strip())\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b7b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Tokenizer charg√© : <class '__main__.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# üß† Classe Tokenizer √† copier (m√™me que dans 03_)\n",
    "class Tokenizer:\n",
    "    def __init__(self, word2idx):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.start_token = \"<start>\"\n",
    "        self.end_token = \"<end>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.pad_token_id = self.word2idx[self.pad_token]\n",
    "        self.start_token_id = self.word2idx[self.start_token]\n",
    "        self.end_token_id = self.word2idx[self.end_token]\n",
    "        self.unk_token_id = self.word2idx[self.unk_token]\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def encode(self, caption, add_special_tokens=True):\n",
    "        tokens = caption.strip().split()\n",
    "        token_ids = [self.word2idx.get(token, self.unk_token_id) for token in tokens]\n",
    "        if add_special_tokens:\n",
    "            return [self.start_token_id] + token_ids + [self.end_token_id]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids, remove_special_tokens=True):\n",
    "        words = [self.idx2word.get(idx, self.unk_token) for idx in token_ids]\n",
    "        if remove_special_tokens:\n",
    "            words = [w for w in words if w not in [self.pad_token, self.start_token, self.end_token]]\n",
    "        return \" \".join(words)\n",
    "\n",
    "# ‚úÖ Chargement de l'objet\n",
    "import pickle\n",
    "\n",
    "with open(\"../data/vocab/tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(\"üß† Tokenizer charg√© :\", type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f029fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoader pr√™t avec 1265 batchs\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = ImageCaptionDataset(\n",
    "    features_dir=\"../data/processed/features_resnet_global\",\n",
    "    captions_dict=aligned_captions,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=37\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoader pr√™t avec {len(dataloader)} batchs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad494a9e",
   "metadata": {},
   "source": [
    "#### üëÄ 4. (Optionnel) Visualisation d‚Äôun batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b17714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded : [1, 4, 12, 8, 120, 2]\n",
      "Decoded : a man is climbing\n"
     ]
    }
   ],
   "source": [
    "test_caption = \"a man is climbing\"\n",
    "print(\"Encoded :\", tokenizer.encode(test_caption))\n",
    "print(\"Decoded :\", tokenizer.decode(tokenizer.encode(test_caption)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e3f1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch charg√©\n",
      "Features shape : torch.Size([32, 2048])\n",
      "Captions shape : torch.Size([32, 24])\n",
      "\n",
      "üñºÔ∏è Extrait 1 :\n",
      "  ‚û§ Vector shape : torch.Size([2048])\n",
      "  ‚û§ Caption (decoded) : a white and brown spotted dog runs along the snow to catch a ball\n",
      "\n",
      "üñºÔ∏è Extrait 2 :\n",
      "  ‚û§ Vector shape : torch.Size([2048])\n",
      "  ‚û§ Caption (decoded) : two small dogs run across the green grass\n",
      "\n",
      "üñºÔ∏è Extrait 3 :\n",
      "  ‚û§ Vector shape : torch.Size([2048])\n",
      "  ‚û§ Caption (decoded) : a brown dog runs in the grass with one ear up\n",
      "\n",
      "üñºÔ∏è Extrait 4 :\n",
      "  ‚û§ Vector shape : torch.Size([2048])\n",
      "  ‚û§ Caption (decoded) : a tan dog is standing in front of some plants\n",
      "\n",
      "üñºÔ∏è Extrait 5 :\n",
      "  ‚û§ Vector shape : torch.Size([2048])\n",
      "  ‚û§ Caption (decoded) : man with no shirt and <unk> on back airborne with skateboard in hand\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# üß† Chargement d'un batch\n",
    "features_batch, captions_batch, lengths = next(iter(dataloader))\n",
    "\n",
    "print(\"‚úÖ Batch charg√©\")\n",
    "print(\"Features shape :\", features_batch.shape)  # [batch_size, 2048]\n",
    "print(\"Captions shape :\", captions_batch.shape)  # [batch_size, max_seq_len]\n",
    "\n",
    "# üîÅ Affichage al√©atoire de 5 exemples\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(captions_batch) - 1)\n",
    "    caption_ids = captions_batch[idx][:lengths[idx]].tolist()\n",
    "    decoded = tokenizer.decode(caption_ids)\n",
    "\n",
    "    print(f\"\\nüñºÔ∏è Extrait {i+1} :\")\n",
    "    print(f\"  ‚û§ Vector shape : {features_batch[idx].shape}\")\n",
    "    print(f\"  ‚û§ Caption (decoded) : {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f31d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's a beautiful day isn't it\n"
     ]
    }
   ],
   "source": [
    "original = \"It's a beautiful day, isn't it?\"\n",
    "cleaned = clean_caption(original)\n",
    "print(cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
